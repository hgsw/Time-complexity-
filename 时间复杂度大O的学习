算法时间复杂度的由来，为什么要在算法上引入时间复杂度？
比如这段代码
// 计算 1, 2, 3…n 的累加和
public class Test {
    int calculation(int n) {
        int sum = 0;       
        for ( int i = 1; i <= n; ++i) 
            sum = sum + i;
        return sum;
    }
}
如果我们要测试以上面这段代码的执行效率，该如何去测试呢 ？？
起初，我们能想到最简单最直接的方法就是把代码在机器上跑一遍，通过统计、监控，就能得到这段代码所执行的时间和占用的内存大小。
既然是这样那为什么还要做时间、空间复杂度分析呢？复杂度分析会比我们实实在在跑一遍得到的数据更准确吗？
这种评估算法的执行效率是正确的。但是这种统计方法有非常大的局限性
1 测试结果极度依赖测试环境
2 测试结果受数据规模的影响很大

算法的执行时间等于它所有基本操作执行时间之和， 而一条基本操作的执行时间等于它执行的次数和每一次执行的时间的积，如下：

算法的执行时间 = 操作1 + 操作2 + ... + 操作n
操作的执行时间 = 操作执行次数 * 执行一次的时间

然而存在一个问题，不同的编程语言，不同的编译器，或不同的CPU等因素将导致执行一次指令操作的时间各不相同，这样的结果会使算法的比较产生歧义， 
于是我们假定所有计算机执行相同的一次指令操作所需时间相同，统一定为 unit_time 。而把算法中基本操作所执行的 执行次数n 作为量度。
就是说我们把算法的 运行时间 简单地用基本操作的 运行次数 来代替了, 进而将分析 运行时间 转移到某一行代码的 运行次数。

那就是 所有代码的执行时间 T(n) 与每行代码的执行次数 n 成正比。

T(n) = O(f(n))
T(n)表示代码的执行时间；n表示数据规模的大小；f(n)是一个函数，表示每行代码执行的次数总和。
函数f(n)外部的大O表示代码的执行时间 T(n) 与 f(n) 表达式成正比。

注意：大 O 时间复杂度实际上并不具体表示代码真正的执行时间，而是表示代码执行时间随数据规模增长的变化趋势，所以，
也叫作渐进时间复杂度(aymptotic time complexity)，简称时间复杂度。或者说它表达的是随着数据规模n的增长，代码执行时间的增长趋势(类似于物理中的加速度)
由此我们可以看出，当我们以增长率的角度去观察 f(n) 函数的时，有几件事就变得很明显：

    首先，我们可以忽略常数项，因为随着n的值变得越来越大，常数项最终变得可忽略不计
    其次，我们可以忽略系数
    最后，我们只需要考虑高阶项的因子即可，不需要考虑低阶项
    
----------------------------------------------------------------------------------------------   
时间复杂度简单规则：

前面介绍了大 O 时间复杂度的由来和表示方法。现在我们来看下，当我们拿到一段代码时，如何去分析这一段代码的时间复杂度？
以下是几个比较实用的方法，我们可以参考一下：

1 只关注循环执行次数最多的一段代码
我刚才说了，大O这种复杂度表示方法只是表示一种变化趋势。我们通常会忽略掉公式中的常量、低阶、系数，只需要记录一个最大阶的量级就可以了。
所以，我们在分析一个算法、一段代码的时间复杂度的时候，也只关注循环执行次数最多的那一段代码即可。这段代码执行次数的n的量级，
就是争端要分析代码的时间复杂度。为了便于你理解，我还拿前面的例子来说明。

int calculation(int n) {
   int sum = 0;
   int i = 1;
   for (; i <= n; ++i) 
     sum = sum + i;
   return sum;
}

其中第 55、56 行代码都是常量级的执行时间，与 n 的大小无关，所以对于复杂度并没有影响。循环执行次数最多的是第 57、58行代码，
所以这块代码要重点分析。前面我们也讲过，这两行代码被执行了 n 次，所以总的时间复杂度就是 O(n)。

----------------------------------------------------------------------------------------------  
2 加法法则：总复杂度登记量级为最大的那段代码的复杂度

----------------------------------------------------------------------------------------------  
3 乘法法则：嵌套代码的复杂度等于嵌套内外代码复杂度的乘积
我们刚讲了一个复杂度分析中的加法法则，这儿还有一个乘法法则。
当分析一个算法的运行时间时，如果一个任务的执行引起了另一个任务的执行，可以运用此规则。
例如，在一个嵌套循环中，外层迭代为T1, 内层迭代为T2, 如果T1 = m, T2 = n, 那么运行结果表示为O(m * n)。
int cal(int n) {
   int ret = 0;
   for (int i = 1; i < n; ++i) 
     ret = ret + f(i);
}
int f(int n) {
  int sum = 0; 
  for ( int i = 1; i < n; ++i) 
    sum = sum + i;
  return sum;
}

我们单独看 cal() 函数。假设 f() 只是一个普通的操作，那第 78-79 行的时间复杂度就是，T1(n) = O(n)。但 f() 函数本身不是一个简单的操作，
它的时间复杂度是 T2(n) = O(n)，所以，整个 cal() 函数的时间复杂度就是，
T(n) = T1(n) * T2(n) = O(n*n) = O(n²)。


原文链接：https://blog.csdn.net/zxm317122667/article/details/94591135




