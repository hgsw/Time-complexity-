算法时间复杂度的由来，为什么要在算法上引入时间复杂度？
比如这段代码
// 计算 1, 2, 3…n 的累加和
public class Test {
    int calculation(int n) {
        int sum = 0;       
        for ( int i = 1; i <= n; ++i) 
            sum = sum + i;
        return sum;
    }
}
如果我们要测试以上面这段代码的执行效率，该如何去测试呢 ？？
起初，我们能想到最简单最直接的方法就是把代码在机器上跑一遍，通过统计、监控，就能得到这段代码所执行的时间和占用的内存大小。
既然是这样那为什么还要做时间、空间复杂度分析呢？复杂度分析会比我们实实在在跑一遍得到的数据更准确吗？
这种评估算法的执行效率是正确的。但是这种统计方法有非常大的局限性
1 测试结果极度依赖测试环境
2 测试结果受数据规模的影响很大

算法的执行时间等于它所有基本操作执行时间之和， 而一条基本操作的执行时间等于它执行的次数和每一次执行的时间的积，如下：

算法的执行时间 = 操作1 + 操作2 + ... + 操作n
操作的执行时间 = 操作执行次数 * 执行一次的时间

然而存在一个问题，不同的编程语言，不同的编译器，或不同的CPU等因素将导致执行一次指令操作的时间各不相同，这样的结果会使算法的比较产生歧义， 
于是我们假定所有计算机执行相同的一次指令操作所需时间相同，统一定为 unit_time 。而把算法中基本操作所执行的 执行次数n 作为量度。
就是说我们把算法的 运行时间 简单地用基本操作的 运行次数 来代替了, 进而将分析 运行时间 转移到某一行代码的 运行次数。

